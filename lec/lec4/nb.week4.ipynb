{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b070164",
   "metadata": {
    "id": "5KwIS43rETr3"
   },
   "source": [
    "# Ch 4 - Inferential Statistics \n",
    "\n",
    "We wish to infer parameters of, and draw conclusions about a population using a statistic (number/property describing a characteristic of a sample).  \n",
    "\n",
    "Goal for this module:\n",
    "* Understanding sampling distributions\n",
    "* Method 1: Point estimates\n",
    "* Method 2: Confidence intervals\n",
    "* Method 3: Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a36c308",
   "metadata": {
    "id": "AC7oOu5vEw2K",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl \n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd458ad8",
   "metadata": {
    "id": "v7poZVVcOY9V"
   },
   "source": [
    "## Populations \n",
    "\n",
    "* Two kinds of distributions: populations and samples \n",
    "* A population is the set of all relevant measurements.  Think of it as the big picture. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f9acc",
   "metadata": {
    "id": "bVwluLAHOpv7"
   },
   "source": [
    "### Population: Finite or Infinite? \n",
    "\n",
    "A population can have a finite number of outcomes, but an infinite extent. \n",
    "\n",
    "**Example**  Consider the set of all possible two-dice throws:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12].  \n",
    "\n",
    "We can ask what the distribution across the totals would be if you threw a pair of dice an infinite number of times. \n",
    "\n",
    "--- \n",
    "\n",
    "Alternatively, a population can also have an infinite number of outcomes and an infinite extent. \n",
    "\n",
    "**Example** Consider a simulation that produces a predicted global average temperature for 2050.  \n",
    "\n",
    "The simulation won't give the same result every time it is run:  15.17, 14.81, 15.02, 14.46, ... \n",
    "\n",
    "We can ask how the values would be distributed across an infinite number of runs of the simulation, each linked to a different sequence of pseudo-random numbers. \n",
    "\n",
    "--- \n",
    "\n",
    "A population can be finite but large. \n",
    "\n",
    "* The set of all fish in the Pacific Ocean \n",
    "* The set of all people currently living in the US \n",
    "\n",
    "A population can be finite and small. \n",
    "\n",
    "* The set of Nobel prize winners born in Hungary. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765dac40",
   "metadata": {
    "id": "WowT7CKuP3V8"
   },
   "source": [
    "### Known Population Distributions \n",
    "\n",
    "Many times our knowledge of probability allows us to specify exactly what the infinite long-run distribution of some process looks like.  \n",
    "\n",
    "We can illustrate this with a probability density function, e.g., a histogram that describes the probability of an outcome rather than counting occurances of that outcome. \n",
    "\n",
    "For the case of totals of two dice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6dcc4",
   "metadata": {
    "id": "wngL72nGQPWF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2022)\n",
    "n = 10000\n",
    "obs1 = rng.choice(range(1,7), n)\n",
    "obs2 = rng.choice(range(1,7), n)\n",
    "tot = obs1 + obs2 \n",
    "plt.hist(tot, bins=[2,3,4,5,6,7,8,9,10,11,12,13], \n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xticks(range(2,13));\n",
    "plt.title('Histogram of dice rolls')\n",
    "plt.xlabel('Total of two dice')\n",
    "plt.ylabel('Probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33137934",
   "metadata": {
    "id": "oa__S495VJHG"
   },
   "source": [
    "#### The need for sampling \n",
    "\n",
    "More commonly, we don't know the precise shape of the population's distribution on some variable.  But we'd *like* to know. \n",
    "\n",
    "We have no alternative but to **sample** the population in some way.  \n",
    "\n",
    "This may mean an empirical sampling.  For example, we go out into the middle of the Pacific Ocean and catch 100 fish in order to learn about the distribution of fish weights. Or, it may mean sampling from many repeated runs of a simulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ddb9e",
   "metadata": {
    "id": "PSbK1H1-VwJV"
   },
   "source": [
    "## Samples \n",
    "\n",
    "A sample is just a group of observations drawn in some way from a wider population.  \n",
    "\n",
    "Statistics has its roots in the effort to figure out just what you can reasonably infer about this wider population from the sample you've got. \n",
    "\n",
    "The *size* of your sample turns out to be an important limiting factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24cb3f",
   "metadata": {
    "id": "x5wh4dePWP3l"
   },
   "source": [
    "### Sampling from a known distribution \n",
    "\n",
    "Let's examine a simple distribution to understand the effects of sampling.  \n",
    "\n",
    "Our distribution is the results from throwing a single die, that is, the uniform distribution across the integers 1 to 6 inclusive. \n",
    "\n",
    "We know that the mean of the distribution is 3.5000, the variance is 2.917, and the standard deviation is 1.708. \n",
    "\n",
    "$$ Mean = (1 + 2 + 3+ 4 + 5 + 6) / 6 = 3.5 $$\n",
    "\n",
    "$$ Variance = ((1-3.5)^2 + (2 - 3.5)^2 + ... (6-3.5)^2) / 6 = 2.917$$\n",
    "\n",
    "$$ Std. Dev. = \\sqrt{Variance} = 1.708$$\n",
    "\n",
    "We can simulate drawing some samples from this distribution to see how the size of our sample affects our attempts to draw conclusions about the population.  \n",
    "\n",
    "First, we create a large sample $n$ = 50000 which will approach the uniform distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08566ab5",
   "metadata": {
    "id": "V9kBaBTeXLC3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 50000\n",
    "obs = rng.choice(range(1,7), n)\n",
    "plt.hist(obs, bins=[1,2,3,4,5,6,7], \n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xticks(range(1,7));\n",
    "plt.title('Probability of Die Outcomes')\n",
    "plt.xlabel('Throw of a single die')\n",
    "plt.ylabel('Probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed56aca",
   "metadata": {
    "id": "4emvYdFhYByN"
   },
   "source": [
    "Let's look at two small samples, one with $n$ = 3 and one with $n$ = 25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2805589",
   "metadata": {
    "id": "Z-bkd7CUYRGu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n1 = 3\n",
    "n2 = 25\n",
    "obs1 = rng.choice(range(1,7), n1)\n",
    "obs2 = rng.choice(range(1,7), n2)\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(obs1, bins=[1,2,3,4,5,6,7], color='red',\n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xticks(range(1,7));\n",
    "plt.xlabel('Throw of a single die')\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Sample size = 3')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(obs2, bins=[1,2,3,4,5,6,7], color='red',\n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xticks(range(1,7));\n",
    "plt.xlabel('Throw of a single die')\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Sample size = 25');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3ce47",
   "metadata": {
    "id": "jcnypU3RY1mC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Mean of sample with  3 obs: %.3f\" % obs1.mean())\n",
    "print(\"Mean of sample with 25 obs: %.3f\" % obs2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2762b",
   "metadata": {
    "id": "fCy_Ja7RZCxH"
   },
   "source": [
    "In both cases, we didn't reproduce the shape of the true distribution, but get close to 3.5 as the mean. \n",
    "\n",
    "The larger sample gave us a more accurate estimate of the population mean.  *Hopefully, not too suprising* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a705b1a3",
   "metadata": {
    "id": "LJLtgRDncJzD"
   },
   "source": [
    "### Sampling Distribution of the Mean\n",
    "\n",
    "Let's simulate drawing a size 3 sample 10,000 time, calculate the sample mean, and see what this distribution looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c94ee3",
   "metadata": {
    "id": "EBDKJlsKcVkK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_tests = 10000\n",
    "n = 3\n",
    "means = [0] * N_tests\n",
    "for i in range(N_tests):\n",
    "    obs = rng.choice(range(1,7), n)\n",
    "    means[i] = obs.mean()\n",
    "    # print (\"Sample \" + str(i) + \", Mean: \", '%.2f' % means[i])\n",
    "print(\"Mean of Sample Means: %.3f\" % np.mean(means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224a985",
   "metadata": {
    "id": "kVkor9epcvUR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(means, color='green',\n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xticks(range(1,7));\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Distribution of means of 10,000 samples of size 3');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b15a3",
   "metadata": {
    "id": "FfCGVcIrdQN-"
   },
   "source": [
    "For sample size 3, it looks like the *mean of the sample means* is close to the true mean of 3.5. \n",
    "\n",
    "But there is a lot of variation.  With such a small sample size, we can get extreme results such as a sample mean of 1 or 6 reasonably often. \n",
    "\n",
    "Do things improve if we look at the distribution of the sample means of sample size 25? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546720c",
   "metadata": {
    "id": "X32xLW_JeKiJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_tests = 10000\n",
    "n = 25\n",
    "means = [0] * N_tests\n",
    "for i in range(N_tests):\n",
    "    obs = rng.choice(range(1,7), n)\n",
    "    means[i] = obs.mean()\n",
    "    # print (\"Sample \" + str(i) + \", Mean: \", '%.2f' % means[i])\n",
    "print(\"Mean of Sample Means: %.3f\" % np.mean(means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dde131",
   "metadata": {
    "id": "KnaS-lzPeRjE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(means, color='green',\n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xticks(range(1,7));\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Distribution of means of 10,000 samples of size 25');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09171113",
   "metadata": {
    "id": "7xDM_PkHesgR"
   },
   "source": [
    "What about distribution of the sample means of sample size 100? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b480d",
   "metadata": {
    "id": "4Hheqgm9erLp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_tests = 10000\n",
    "n = 100\n",
    "means = [0] * N_tests\n",
    "for i in range(N_tests):\n",
    "    obs = rng.choice(range(1,7), n)\n",
    "    means[i] = obs.mean()\n",
    "    # print (\"Sample \" + str(i) + \", Mean: \", '%.2f' % means[i])\n",
    "print(\"Mean of Sample Means: %.3f\" % np.mean(means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25c489",
   "metadata": {
    "id": "K_RCWQ_Le41g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(means, color='green',\n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xticks(range(1,7));\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Distribution of means of 10,000 samples of size 100');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb370b8d",
   "metadata": {
    "id": "NQwfOSTenzo6"
   },
   "source": [
    "What do we see from these figures? \n",
    "\n",
    "* The distribution of the sample means looks like it is shaped like a bell curve, despite the fact that we've been sampling from a uniform distribution \n",
    "\n",
    "* The width of the bell curve is getting gradually smaller as the size of our samples increases.  So bigger samples seem to give tighter, more accurate estimates. \n",
    "\n",
    "* Even for really small sample sizes, like 3, the sample mean distribution looks like it is *centered* on the true mean, but for a particular sample we could be way off. \n",
    "\n",
    "What do we know? \n",
    "\n",
    "* The mean of our sample means will converge on the true mean. \n",
    "\n",
    "* The standard deviation of our distribution of *sample means* will tighten up in proportion to $\\frac{1}{\\sqrt{N}}$.  In other words, accuracy improves with bigger sample sizes, but with diminishing returns.  This $\\frac{1}{\\sqrt{N}}$ ratio is related to the *standard error*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b3dad",
   "metadata": {
    "id": "ZOz8YTUdETr5"
   },
   "source": [
    "## Samples vs. Populations: Red Wine Example\n",
    "\n",
    "Lets return to the red wine quality data set, and try to better understand samples versus population.  There are two different viewpoints, both valid:\n",
    "* We have only *sampled* some of red wines produced in Northern Portugal.  Hence, the data set we have is a *sample*, which we can use to infer characteristics about red wines produced in Portugal.\n",
    "* We can think of our data set as the population, e.g., the data covers all the varieties of red wines produced at wineries in Portugal in a certain month.  \n",
    "\n",
    "We shall take the latter interpretation for the rest of the lecture. Lets begin by importing our data set as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a5e5c",
   "metadata": {
    "id": "zCJrnXGrETr6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wine = pd.read_csv(url, sep=\";\")\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d2313-d14d-4f9c-86fe-71659acd8b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wine.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace67b9",
   "metadata": {
    "id": "Q5Bkpus5ETr9"
   },
   "source": [
    "We learned how to find the mean pH of the wine last week.  \n",
    "\n",
    "Let's call this the population mean, $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624ecf5",
   "metadata": {
    "id": "FnfPD4vvETr-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu = wine[\"pH\"].mean()\n",
    "print(\"mean pH, mu = \", str(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580aabe2",
   "metadata": {
    "id": "66zWPixzETsB"
   },
   "source": [
    "Perhaps, someone did not believe that red wine was so acidic, and demanded that we re-measure the pH of the wines.  This might be ridiculously time consuming/expensive, and we don't want to take the time to sample all 1599 red wines again.  \n",
    "\n",
    "Instead, what we want to do is take a few random samples of the wine, and convince the appropriate party that the reported mean pH was reasonable.  How do we do that?  How do we quantify our confidence in the reported pH? To do this, we first need to understand how meaningful it is to take the mean of samples.  A key concept is *the sampling distribution of the sample mean*.\n",
    "\n",
    "Suppose that our initial measurements were actually accurate (i.e., if we measured the same wine again, the reported pH will be the same value.  Suppose we were only interested in re-measuring the mean pH of $n=30$ wines.  If we drew all possible samples of size $n=30$, measured the mean of each sample, then the probability distribution of this mean is called the *mean sampling distribution*.  For this example, there are a total of $1599 choose 30$ samples. This is a very large number of samples.  We can instead approximate the mean sampling distribution by drawing a large number of samples of size $n=30$, say 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e4c4c",
   "metadata": {
    "id": "Vf5gUp9dETsB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_tests = 1000\n",
    "n = 30\n",
    "means = [0] * N_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429400e",
   "metadata": {
    "id": "ytMQRQyUETsE"
   },
   "source": [
    "Let's set up this testing, the last command above generates list with $N_{test}$ elements, each element initialized to 0.  Let's now generate each sample and store the mean of the sample in the new list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96117b",
   "metadata": {
    "id": "BrsHJLe0ETsF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(N_tests):\n",
    "    obs = np.random.choice(wine.index.values, n)\n",
    "    sample_wines = wine.loc[obs]\n",
    "    means[i] = sample_wines[\"pH\"].mean()\n",
    "    print (\"Sample \" + str(i) + \", Mean: \", '%.2f' % means[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf10b5",
   "metadata": {
    "id": "TjQRsgsMETsH"
   },
   "source": [
    "Now, we can generate a histogram of the means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d961e",
   "metadata": {
    "id": "eKWOc_OSETsI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nbins = 30\n",
    "_ = plt.hist(means, nbins, density=True)\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Sample mean');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f03254",
   "metadata": {
    "id": "DyQOWQRXb2AP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Mean of the Sample Means: %.3f' % np.mean(means))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02653e",
   "metadata": {
    "id": "gRlSC7iTETsL"
   },
   "source": [
    "### Central Limit Theorem \n",
    "\n",
    "The *central limit theorem* tells us that we reliably get a normal distribution when we look at the distribution of sample means, no matter what the original distribution that we were sampling from. \n",
    "\n",
    "Let's fit a normal distribution to this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12208ca4",
   "metadata": {
    "id": "hFZ34CVfETsM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "xbar, s = norm.fit(means)\n",
    "print(\"mean = %g, standard deviation of distribution = %g\"%(xbar, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54b5b1",
   "metadata": {
    "id": "fIRq2wdLETsP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(means, bins=nbins, density=True)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin,xmax,100)\n",
    "p = norm.pdf(x, xbar, s)\n",
    "_ = plt.plot(x, p, 'r', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b6e46",
   "metadata": {
    "id": "JeWlW43yETsS"
   },
   "source": [
    "So, our sampling distribution is well approximated by the Gaussian (normal) distribution.  Lets review some properties of the normal distribution:\n",
    "* the distribution is symmetric about it's mean;\n",
    "* there is a single peak, mean = median = mode (most frequently occurring value in series), located at $x = \\mu$;\n",
    "* the distribution has inflection points at $\\mu \\pm \\sigma$, where $\\sigma$ is the standard deviation;\n",
    "* The area under the distribution is 1;\n",
    "* The area under the curve to the left (right) of $\\mu$ is 0.5;\n",
    "* The curve approaches, but never reaches the horizontal axis.\n",
    "\n",
    "We already saw this last week, the normal distribution can be described mathematically by\n",
    "$$ N(\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left(-\\frac{(x-\\mu)^2}{2 \\sigma^2} \\right).$$\n",
    "Often, we standardize normal data by computing z-scores, so that any Normal curve $N(\\mu,\\sigma^2)$ can be transformed into the standard normal curve $N(0,1)$,\n",
    "$$z = \\frac{x-\\mu}{\\sigma}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc37ce",
   "metadata": {
    "id": "35-K6LrYqsBU"
   },
   "source": [
    "## The Normal Distribution \n",
    "\n",
    "The normal distribution is central to statistical inference.  \n",
    "\n",
    "A particular normal distribution is fully characterized by just two parameters: the mean, $\\mu$, and the standard deviation, $\\sigma$ \n",
    "\n",
    "<img src=\"images/normal-distribution.png\" width=\"750px\">\n",
    "\n",
    "The normal distribution's consistent shape is useful because we can say precise things about areas under the curve. \n",
    "\n",
    "It's a probablity distribution so the area sums to 1 (or 100%). \n",
    "\n",
    "* 68% of the time the variate will be within plus or minus one standard deviation of the mean (i.e., a z-score between -1 and 1) \n",
    "* 95% of variates will be within two standard deviations. \n",
    "* 99.7% of variates will be within three standard deviations. \n",
    "\n",
    "<img src=\"images/standard-deviation.png\" width=\"750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec7e49",
   "metadata": {
    "id": "WKdSo35itAJy"
   },
   "source": [
    "### Example of Normal Variates \n",
    "\n",
    "Suppose we have a normal distribution with a mean of 100 and a standard deviation of 10.  We can reason about values. \n",
    "\n",
    "* Only 0.1% of cases will have a score higher than 130.  \n",
    "* About 95% of cases will lie between 80 and 120.  \n",
    "* About 34% of cases will be between 100 and 110.  \n",
    "\n",
    "More on this later ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36bc17",
   "metadata": {
    "id": "mFsZ8R7zETsS"
   },
   "source": [
    "## Method 1: Point Estimates \n",
    "\n",
    "We are now ready to describe Method 1: using sample data to give a point estimate (best guess) of the population parameters.  The idea is to take a sample of the population, e.g. $n=30$ wines, and compute the mean of the sample, $\\bar{x}$.  Then, one estimates the population mean, $\\mu$, using:\n",
    "$$ \\mu = \\bar{x}.$$\n",
    "\n",
    "We can also estimate $\\sigma_{\\bar{x}}$ the standard deviation of the sample mean , or *standard error* as:\n",
    "$$ SE = \\frac{\\sigma_x}{\\sqrt{n}},$$ \n",
    "where $\\sigma$ is the standard deviation of the population. If the standard deviation of the population is not known, one can use the sample standard deviation if the population distribution is not skewed, and if $n>30$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4965c",
   "metadata": {
    "id": "qoWR0qPqETsT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 30\n",
    "obs = np.random.choice(wine.index.values,n)\n",
    "sampled_wines = wine.loc[obs]\n",
    "xbar = sampled_wines[\"pH\"].mean()\n",
    "sigma = wine[\"pH\"].std()\n",
    "se = sigma/np.sqrt(n)\n",
    "est_se = sampled_wines[\"pH\"].std()/np.sqrt(30)\n",
    "print (\"Estimate of population mean = %g, standard error = %g\"%(xbar,se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c86b0",
   "metadata": {
    "id": "7ufpQdSFETsV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (\"Standard error using population standard deviation:\", '%04f' \n",
    "       % se)\n",
    "print (\"Standard error estimate from one sample with 30 elements:\", '%04f' \n",
    "       % est_se)\n",
    "print (\"Standard error estimate from 1000 samples with 30 elements:\", \"%04f\" \n",
    "       % np.array(means).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e7119",
   "metadata": {
    "id": "Ap1y4-InETsX"
   },
   "source": [
    "### Bootstrapping \n",
    "\n",
    "A modern alternative to the traditional approach to statistical inference is the bootstrapping method. In the bootstrap, we draw $N$ observations with replacement from the original data to create a bootstrap sample or resample. Then, we can calculate the mean for this resample. By repeating this process a large number of times we can built a good approximation of the mean sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38e30f",
   "metadata": {
    "id": "DoqRIk_4ETsY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def meanBootstrap(X,numberb):\n",
    "    x = [0]*numberb\n",
    "    for i in range(numberb):\n",
    "        if i % 100 == 0:\n",
    "            print(str(i) + \" \", end='')\n",
    "        if i % 1000 == 0:\n",
    "            print(\"\")\n",
    "        sample = [X[_] for _ in np.random.randint(len(X), size=len(X))]\n",
    "        x[i] = np.mean(sample)\n",
    "    return x\n",
    "\n",
    "m = meanBootstrap(wine[\"pH\"], 2000)\n",
    "print (\"\\nMean estimate:\", np.mean(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e0060",
   "metadata": {
    "id": "1yt6Tzk5ETsc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_=plt.hist(m, bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b7feee",
   "metadata": {
    "id": "er0EVyWpETsf"
   },
   "source": [
    "Bootstrapping can also be used to estimate other statistics such as median or variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008ff27",
   "metadata": {
    "id": "YF7q3s_METsf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def medBootstrap(X,numberb):\n",
    "    x = [0]*numberb\n",
    "    for i in range(numberb):\n",
    "        sample = [X[_] for _ in np.random.randint(len(X), size=len(X))]\n",
    "        x[i] = np.median(sample)\n",
    "    return x\n",
    "\n",
    "med = medBootstrap(wine['pH'], 1000)\n",
    "print (\"Median estimate:\", np.mean(med))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5b067",
   "metadata": {
    "id": "nf5WqM2NETsi"
   },
   "source": [
    "## Method 2: Confidence Intervals \n",
    "\n",
    "A point estimate, such as a sample mean, provides a single plausible value for a parameter. However, as we have seen a point estimate is rarely perfect; usually there is some error in the estimate. That is why we have proposed to use the standard error as a measure of its variability.\n",
    "\n",
    "As an alternative, a next logical step would be to provide a **plausible range of\n",
    "values** for the parameter. A plausible range of values for the sample parameter\n",
    "is called a **confidence interval**.\n",
    "\n",
    "We will base the definition of confidence interval on two ideas:\n",
    "\n",
    "+ Our point estimate is the most plausible value of the parameter, so it makes sense to build the confidence interval around the point estimate.\n",
    "\n",
    "+ The plausability of a range of values can be defined from the sampling distribution of the estimate.\n",
    "\n",
    "In order to define an interval, we can make use of a well\n",
    "known result from probability that applies to normal distributions: \n",
    "\n",
    "* roughly 95% of the time our estimate will be within 1.96 standard errors of the true mean of the distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c2814",
   "metadata": {
    "id": "_nZqLK29ETsj"
   },
   "source": [
    "**Details on Confidence Intervals**\n",
    "\n",
    "The $100\\cdot(1-\\alpha)\\%$ confidence interval for $\\mu$ is,\n",
    "$$ \\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}},$$\n",
    "i.e, we use the point estimate for the mean, the $z$-score which determines the confidence interval, and the standard error of the mean.  Lets try and understand this formula by starting with the standard normal (using the $z$-score), and then transforming to the problem at hand.  Suppose we want the confidence level = $C\\% = (1-\\alpha)100 \\%$.  Then\n",
    "\\begin{align}\n",
    "C &= (1-\\alpha)100 \\\\\n",
    "&=P(-z_{\\alpha/2} \\le Z \\le z_{\\alpha/2}) \\\\\n",
    "&= P(-z_{\\alpha/2} \\le \\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}} \\le z_{\\alpha/2})\\\\\n",
    "&= P\\left(-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\le (\\bar{x}-\\mu) \\le z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\\\\n",
    "&= P\\left(-\\bar{x}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\le -\\mu \\le -\\bar{x} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\\\\n",
    "&= P\\left(\\bar{x}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\ge \\mu \\ge \\bar{x} - z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\\\\n",
    "&= P\\left(\\bar{x}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\le \\mu \\le \\bar{x} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f85eb",
   "metadata": {
    "id": "D05LpMGMETsk"
   },
   "source": [
    "For our example, if we want a 95% confidence interval, this means $\\alpha=0.05$, which (from a z-table) gives $z_{\\alpha/2} = 1.96$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7189d",
   "metadata": {
    "id": "CWd-JQlcETsl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "z_alphadiv2 = norm.ppf(1-alpha/2)\n",
    "confidence = norm.cdf(z_alphadiv2)-norm.cdf(-z_alphadiv2)\n",
    "ci = [xbar - se*z_alphadiv2, xbar + se*z_alphadiv2 ]\n",
    "print (\"We are\", round(100*confidence,4), \"% confident that the interval = \",  \n",
    "       ci,  \"contains the mean.\")\n",
    "z_alphadiv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817721ee",
   "metadata": {
    "id": "CcLbw5LAHnU3"
   },
   "source": [
    "Let's look more closely at this code in what it is doing.  First, we are looking at 95% confidence, thus setting `alpha` = 0.05.  We can use the `norm.ppf` function - the percent point function in `scipy`.   This call is equivalent to looking up in a textbook or other reference what the z-value is.  As suggested above for 95% confidence this should be ~1.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d499b",
   "metadata": {
    "id": "AbN4EeEwH-Zx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(norm.ppf(1-alpha/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34f754",
   "metadata": {
    "id": "hYwuV3UsIS63"
   },
   "source": [
    "Why do we use `alpha/2` in this call?  This is because we are looking a distribution with two tails (the spread in both directions). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60ba90",
   "metadata": {
    "id": "kP-LS7JWIgDr"
   },
   "source": [
    "The variable `ci` holds the confidence intervals, the lower and upper value (or range) which should contain the mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130ad9a",
   "metadata": {
    "id": "EtczMWFMETsn"
   },
   "source": [
    "**WARNING**: what does this 95% confident mean? We cannot say that our specific sample has a 95% chance of containing the true parameter.    Rather, a correct interpretation is, if we were to take 100 samples and compute 100 confidence intervals, 95% of the confidence intervals are likely to contain the true mean of the population.  Lets explore and see:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12bf085",
   "metadata": {
    "id": "1qYNHIAUETsn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_test = 100\n",
    "n = 30\n",
    "means = np.array([0.0] * N_test)   # initialize list for means\n",
    "sigma = np.array([0.0] * N_test)   # initialize list for standard deviation\n",
    "ci = np.array([[0.0,0.0]] * N_test)  # initialize list for confidence intervals\n",
    "mu = wine[\"pH\"].mean()  # true mean\n",
    "for i in range(N_test):\n",
    "    observations = np.random.choice(wine.index.values,n)\n",
    "    sampled_wines = wine.loc[observations]\n",
    "    means[i] = sampled_wines[\"pH\"].mean()\n",
    "    sigma[i] = sampled_wines[\"pH\"].std()\n",
    "    ci[i] = means[i] + np.array([-sigma[i] * z_alphadiv2/np.sqrt(n), \n",
    "                                 sigma[i]*z_alphadiv2/np.sqrt(n)])\n",
    "\n",
    "out1 = ci[:,0] > mu # flag CI that do not contain the \"true\" mean\n",
    "out2 = ci[:,1] < mu # flag CI that do not contain the \"true\" mean\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "ind = np.arange(1, N_test+1)\n",
    "ax.axhline(y = mu, \n",
    "           xmin = 0, \n",
    "           xmax = N_test+1, \n",
    "           color = [0, 0, 0])\n",
    "\n",
    "ci = np.transpose(ci)\n",
    "ax.plot([ind,ind], \n",
    "        ci, \n",
    "        color = '0.75', \n",
    "        marker = '_', \n",
    "        ms = 0, \n",
    "        linewidth = 3)\n",
    "ax.plot([ind[out1],ind[out1]], \n",
    "        ci[:, out1], \n",
    "        color = [1, 0, 0, 0.8], \n",
    "        marker = '_', \n",
    "        ms = 0, \n",
    "        linewidth = 3)\n",
    "ax.plot([ind[out2],ind[out2]], \n",
    "        ci[:, out2], \n",
    "        color = [1, 0, 0, 0.8], \n",
    "        marker = '_',\n",
    "        ms = 0, \n",
    "        linewidth = 3)\n",
    "ax.plot(ind, \n",
    "        means, \n",
    "        color = [0, .8, .2, .8], \n",
    "        marker = '.',\n",
    "        ms = 10, \n",
    "        linestyle = '')\n",
    "ax.set_ylabel(\"Confidence interval for the samples' mean estimate\",\n",
    "              fontsize = 12)\n",
    "ax.set_xlabel('Samples (with %d observations). '  %n, \n",
    "              fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43485e3",
   "metadata": {
    "id": "f1eCgukRETsq"
   },
   "source": [
    "If the population standard deviation, $\\sigma$, is unknown, then we need to estimate the population standard deviation.  Compare:\n",
    "\\begin{align}\n",
    "  Z = \\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\n",
    "\\end{align}\n",
    "and\n",
    "\\begin{align}\n",
    "T = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\sim t(df = n-1)\n",
    "\\end{align}\n",
    "Lets compare the $T$-distribution to the normal distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97f8f8",
   "metadata": {
    "id": "NIjnxWvxETsr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "xmin = -10\n",
    "xmax = 10\n",
    "x = np.linspace(xmin,xmax,100)\n",
    "p = norm.pdf(x, 0, 1)\n",
    "_ = plt.plot(x, p, 'r', linewidth = 2,label=\"z-curve\")\n",
    "_ = plt.legend()\n",
    "\n",
    "from scipy.stats import t\n",
    "dof = 29;\n",
    "q = t.pdf(x,dof)\n",
    "_ = plt.plot(x, q, 'b', linewidth = 2,label=\"T, dof=%d\"%(dof))\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4efe203",
   "metadata": {
    "id": "XJX9ZpVoETsu"
   },
   "source": [
    "Like the normal distribution, the T-distribution is symmetric about the mean. However, it is shorter and wider than the Z-curve. It has an extra parameter (Degree of Freedom). In our case, the degree of freedom is one less than the number of observations. If the population standard deviation is not known (often the case), one should use the T-distribution rather than the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51585c91",
   "metadata": {
    "id": "QdgVtvAhETsv"
   },
   "source": [
    "###  Exercise:\n",
    "\n",
    "An engineer working for Ford is interested in the population of all vehicles that have an engine size of 3.0L or larger, and is particular interested in $\\mu$, the highway mileage (mpg). Assume the population is normally distributed. The sample mean among a random sample of 14 vehicles is 18.3 mpg, and the sample standard deviation is 5.1 mpg (note: $\\sigma$ is unknown). What is the 95% CI for $\\mu$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a272a1",
   "metadata": {
    "id": "jafiypEgETsw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the normal distribution, if the population standard deviation is known\n",
    "alpha = 0.05\n",
    "xbar = 18.3 \n",
    "n = 14 \n",
    "se = 5.1/np.sqrt(n)   # if 5.1 was population standard deviation \n",
    "\n",
    "z_alphadiv2 = norm.ppf(1-alpha/2)\n",
    "confidence = norm.cdf(z_alphadiv2) - norm.cdf(-z_alphadiv2)\n",
    "ci = [xbar - se*z_alphadiv2, xbar + se*z_alphadiv2]\n",
    "print (\"We are\", round(100*confidence,4), \"% confident that the interval = \", \n",
    "       ci,  \"contains the mean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976421b2",
   "metadata": {
    "id": "RktQSKk9ETs7"
   },
   "source": [
    "Because we have a sample standard deviation, we need to use t-distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5ab3ca",
   "metadata": {
    "id": "bGKeLCBdETs8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the t distribution, use the sample standard deviation\n",
    "alpha = 0.05\n",
    "xbar = 18.3\n",
    "n = 14\n",
    "dof = n-1\n",
    "se = 5.1/np.sqrt(n) #  5.1 was the sample standard deviation\n",
    "\n",
    "t_alphadiv2 = t.ppf(1-alpha/2,dof)\n",
    "confidence = t.cdf(t_alphadiv2,dof)-t.cdf(-t_alphadiv2,dof)\n",
    "ci = [xbar - se*t_alphadiv2, xbar + se*t_alphadiv2 ]\n",
    "print (\"We are\", round(100*confidence,4), \"% confident that the interval = \", \n",
    "       ci,  \"contains the mean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff085e",
   "metadata": {
    "id": "DKGy8mk7ETs-"
   },
   "source": [
    "What if we are interested in tighter or looser bounds.  Then, we need to change alpha.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d4c6c",
   "metadata": {
    "id": "-QgM7YLAETs-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "xbar = 18.3 \n",
    "n = 14 \n",
    "se = 5.1/np.sqrt(n)   # if 5.1 was population standard deviation \n",
    "\n",
    "z_alphadiv2 = t.ppf(1-alpha/2,dof)\n",
    "confidence = t.cdf(z_alphadiv2,dof) - t.cdf(-z_alphadiv2,dof)\n",
    "ci = [xbar - se*z_alphadiv2, xbar + se*z_alphadiv2]\n",
    "print (\"We are\", 100*confidence, \"% confident that the interval = \",  \n",
    "       ci,  \"contains the mean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca15ce57",
   "metadata": {
    "id": "-RyIWSl-ETtA"
   },
   "source": [
    "#### Z-estimates\n",
    "\n",
    "We saw that a 95\\% confidence interval has a z-value of 1.96. \n",
    "\n",
    "$$ CI = [\\mu - se*1.96,  \\mu + se*1.96] $$\n",
    "\n",
    "To get other confidence intervals, we need to change the z-value (either as above) or using the following table. \n",
    "\n",
    "| Confidence Level | 90%  | 95%  | 99%  | 99.9% |\n",
    "|------------------|------|------|------|-------|\n",
    "| z-value          | 1.65 | 1.96 | 2.58 | 3.291 | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2acaea0",
   "metadata": {
    "id": "sNg9D61cETtB"
   },
   "source": [
    "## Hypothesis Testing \n",
    "\n",
    "Hypothesis: Statement to be tested.  Often, this is referred to as the null hypothesis, $H_0$.  The alternative hypothesis $H_a$ is, as the name suggests, the alternative to null hypothesis (if $H_0$ is not true, what do I suspect might be true instead?) \n",
    "\n",
    "There are two competing camps for hypothesis testing:\n",
    "* Bayesian inference: a probability is assigned/computed for a hypothesis\n",
    "* Frequentist approach: depends on likelihood of observed/unobserved data.\n",
    "\n",
    "### Comment:\n",
    "We will cover (as per the textbook) the frequentist approach, using frequentist measures like $p$-values and confidence intervals.  This has been the dominant approach to conducting hypothesis testing over the last two decades.  I do want to take a few minutes to talk about the Bayesian approach, since more recently, the Bayesian inference (MA 5770) has been garnering enthusiasm in fields like machine learning.  Bayesian inference uses the idea of conditional probability (e.g. P(A|B): probability that event A happens given event B) to determine which hypothesis is most probable.  One has to specify a prior distribution  about the probability distribution that represents the statistic one cares about.  To use Bayesian inference, one needs a full understanding of the statistical model.  Lets return to the frequentist approach for hypothesis testing.\n",
    "\n",
    "### Five-Step Procedure\n",
    "1. Formulate the appropriate hypothesis\n",
    "2. Decide on an appropriate test statistic\n",
    "3. Specify the critical region for the test statistics\n",
    "4. Conduct the experiment and find the specific value for the test statistic\n",
    "5. Reach an appropriate conclusion and state them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07eb2d",
   "metadata": {
    "id": "lXoBJr0YzaOF"
   },
   "source": [
    "### Hypothesis Testing Example \n",
    "\n",
    "Consider the following example: You inherit some\n",
    "money, and you ask a friend who knows about the stock market to invest it for you.  One year later, all the money is gone.  \n",
    "\n",
    "* The null hypothesis: your friend simply had an unlucky run of investments. It could have happened to anyone.\n",
    "* The alternative hypothesis: your friend has cheated you.\n",
    "\n",
    "Consider $H_a$ - cheat. Assume he's cheating you; what are the chances that he'd report a total loss of the money after one year? This is quite hard to answer: it depends on just how sophisticated a cheat he is.  \n",
    "\n",
    "Consider $H_0$ - unlucky.  What are the chances that an honest investor would lose the money in the market as it's been over the last year? A more tractable question...\n",
    "\n",
    "* You could ask some independent experts just how tough the\n",
    "market has been that year.\n",
    "* You could simulate a range of investment strategies using\n",
    "historical market data\n",
    "* You could look empirically at how many people out of the\n",
    "wider population of investors lost all their money over the\n",
    "last year\n",
    "\n",
    "Using one or all of these methods, let's say you find that it's been a very tough year, and in fact there's a 50% chance of an honest investor having lost all their money.\n",
    "\n",
    "It's therefore hard to rule out the null hypothesis. You're\n",
    "forced to conclude something like \"He may well be honest.\"\n",
    "\n",
    "But let's say you find that it's been a great year, and that only 1 honest investor in 1000 lost money.\n",
    "\n",
    "If you want to hang onto the null hypothesis (honesty) under\n",
    "these circumstances, you have to accept that a very unlikely\n",
    "thing has happened.\n",
    "\n",
    "So because of the small probability of the observed data\n",
    "(total loss) given the hypothesis (honesty) you are nudged\n",
    "towards the conclusion that the alternative hypothesis\n",
    "(cheating) is likely to be true.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c91c34",
   "metadata": {
    "id": "xFmG8dWXETtB"
   },
   "source": [
    "### Understanding Hypothesis\n",
    "A common example is drawn from the judicial system: innocent until proven guilty.  Here,\n",
    "* $H_0$: defendant is innocent\n",
    "* $H_a$: defendant is guilty\n",
    "\n",
    "    \n",
    "<table border=1px>\n",
    "    <tr>\n",
    "        <th>Defendant State</th>\n",
    "        <th>Convict (reject $H_0$) </th>\n",
    "        <th>Acquit (do not reject $H_0$)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Innocent ($H_0$ is true)</td>\n",
    "        <td>Type I error</td>\n",
    "        <td> OK</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Guilty ($H_0$ is false)</td>\n",
    "        <td>OK</td>\n",
    "        <td>Type II error</td>\n",
    "    </tr>\n",
    "    </table>\n",
    "\n",
    "* Type I error: mistake of rejecting the null hypothesis when it was in fact true.  The probability of committing a type I error is often denoted by $\\alpha$ (note: overloaded use of variable $\\alpha$).\n",
    "* Type II error: mistake of failing to reject the null hypothesis when it is false.  The probability of committing a type II error is often denoted by $\\beta$.\n",
    "\n",
    "There are several common types of hypothesis:\n",
    "1. Equal versus not equal hypothesis (a.k.a. two-tailed test)\n",
    "    * $H_0$: parameter = some value (e.g. $H_0: \\mu = 17$)\n",
    "    * $H_a$: parameter $\\neq$ some value (e.g. $H_a: \\mu \\neq 17$)\n",
    "2. Equal versus greater than hypothesis (a.k.a. right-tailed test)\n",
    "    * $H_0$: parameter = some value (e.g. $H_0: \\mu = 17$)\n",
    "    * $H_a$: parameter $>$ some value (e.g. $H_a: \\mu > 17$)    \n",
    "3. Equal versus less than hypothesis (a.k.a. left-tailed test)\n",
    "    * $H_0$: parameter = some value (e.g. $H_0: \\mu = 17$)\n",
    "    * $H_a$: parameter $<$ some value (e.g. $H_a: \\mu < 17$)   \n",
    "    \n",
    "### Test Statistic\n",
    "Let $\\mu_0$ be the nominal value for $\\mu$, i.e., for the three types of hypothesis above:\n",
    "1. $H_0: \\mu = \\mu_0$, $H_a: \\mu \\neq \\mu_0$\n",
    "2. $H_0: \\mu = \\mu_0$, $H_a: \\mu > \\mu_0$\n",
    "3. $H_0: \\mu = \\mu_0$, $H_a: \\mu < \\mu_0$\n",
    "\n",
    "If $\\sigma$ (population standard deviation is known, use\n",
    "$$ Z = \\frac{\\bar{x}-\\mu_0}{\\sigma/\\sqrt{n}} \\sim N(0,1). $$\n",
    "Otherwise, if $\\sigma$ is not known, use\n",
    "$$ T = \\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}} \\sim t(df=n-1). $$\n",
    "\n",
    "### Critical Regions\n",
    "two-tail test ($H_0: \\mu = \\mu_0$, $H_a: \\mu \\neq \\mu_0$):  \n",
    "<img src=\"images/two_tail.png\">\n",
    "\n",
    "left-tail test ($H_0: \\mu = \\mu_0$, $H_a: \\mu < \\mu_0$):  \n",
    "<img src=\"images/left_tail.png\">\n",
    "\n",
    "right-tail test ($H_0: \\mu = \\mu_0$, $H_a: \\mu > \\mu_0$):  \n",
    "<img src=\"images/right_tail.png\">\n",
    "\n",
    "### Conduct the experiment\n",
    "Find the specific value of the test statistic.  Note:\n",
    "* One should always complete the first three steps **before** any data is colected\n",
    "* A well-planned experiment should have clear criteria for making decisions before the data collection, in order to ensure objectivity\n",
    "\n",
    "### Conclusions\n",
    "If the test statistic falls inside the critical region, we reject $H_0$\n",
    "* We reject $H_0$ if we have **significant** evidence at level $\\alpha$ that $H_0$ is false\n",
    "* We do not reject $H_0$ if data is **NOT significant** at level $\\alpha$.\n",
    "\n",
    "Lets quantify this.  One often computes a $p$-value, the probability of observing the computed statistic if the null hypothesis is true.  The $p$-value measures, in some sense, how far into the tail we are, based on the computed statistic, i.e., how significant the observation is.  The closer the $p$-value is to zero, the more evidence we have against $H_0$.  We reject $H_0$ if $p$-value $< \\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcc69a",
   "metadata": {
    "id": "qGtZJyh12ZVc"
   },
   "source": [
    "### Example \n",
    "\n",
    "In the terms of our investment example, a statistical test is just a procedure for calculating p(loss | honesty) or its\n",
    "equivalent.\n",
    "\n",
    "The t-test is one such test.\n",
    "\n",
    "We found above that if we take repeated\n",
    "samples from a population, even of quite small size, the\n",
    "distribution of the means of those samples quickly\n",
    "approximates the bell curve of the normal distribution.\n",
    "\n",
    "If we're dealing with big sample sizes, the distribution of the sample means is as close as makes no difference to being\n",
    "the normal distribution.\n",
    "\n",
    "But the match is not perfect for small samples though. This\n",
    "is where the t-distribution comes in.\n",
    "\n",
    "Let's say we have some kind of measure where the true\n",
    "distribution is normal, with a mean of 100 and a standard\n",
    "deviation of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7151df",
   "metadata": {
    "id": "FnG4Drls23UC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs = rng.normal(100,10,10000)\n",
    "plt.hist(obs, bins=20, align='left', edgecolor='black', density=True)\n",
    "plt.xlabel('Measure')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('True Distribution of Measure');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f961e3e4",
   "metadata": {
    "id": "yLSN1RHk3VVK"
   },
   "source": [
    "Let's look at a few small samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad39d1",
   "metadata": {
    "id": "Rza29cIa3dFr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n1 = 2\n",
    "n2 = 3\n",
    "n3 = 6\n",
    "obs1 = rng.normal(100, 10, n1)\n",
    "obs2 = rng.normal(100, 10, n2)\n",
    "obs3 = rng.normal(100, 10, n3)\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(obs1, color='red',\n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xlabel('Measure')\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Sample size = 2')\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(obs2,  color='red',\n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xlabel('Measure')\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Sample size = 3');\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(obs3,  color='red',\n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xlabel('Measure')\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Sample size = 6');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a399333",
   "metadata": {
    "id": "nBUwbKPx4BqN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Mean of 2- Sample:  %.3f\" % obs1.mean())\n",
    "print(\"Mean of 3- Sample:  %.3f\" % obs2.mean())\n",
    "print(\"Mean of 6- Sample:  %.3f\" % obs3.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d8a3c",
   "metadata": {
    "id": "3Ye7lPDn4VJc"
   },
   "source": [
    "What are we doing when we take a sample of size N and\n",
    "calculate the mean?\n",
    "\n",
    "We know that there's a \"meta-distribution\" that describes the mean and standard deviations of the sample means. (Think\n",
    "of the green histograms.)\n",
    "\n",
    "The mean of this meta-distribution is the original population mean, and the standard deviation is the population standard deviation divided by the square root of the sample size (i.e., the standard error).\n",
    "\n",
    "So when we calculate the mean of one sample, we are\n",
    "drawing a random variate from this meta-distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf99b8",
   "metadata": {
    "id": "nm9GZHo-4pPM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_tests = 10000\n",
    "n = 2\n",
    "means1 = [0] * N_tests\n",
    "for i in range(N_tests):\n",
    "    obs = rng.normal(100,10,n)\n",
    "    means1[i] = obs.mean()\n",
    "    # print (\"Sample \" + str(i) + \", Mean: \", '%.2f' % means[i])\n",
    "print(\"Mean of Sample Means: %.3f\" % np.mean(means1))\n",
    "n = 3\n",
    "means2 = [0] * N_tests\n",
    "for i in range(N_tests):\n",
    "    obs = rng.normal(100,10,n)\n",
    "    means2[i] = obs.mean()\n",
    "    # print (\"Sample \" + str(i) + \", Mean: \", '%.2f' % means[i])\n",
    "print(\"Mean of Sample Means: %.3f\" % np.mean(means2))\n",
    "n = 6\n",
    "means3 = [0] * N_tests\n",
    "for i in range(N_tests):\n",
    "    obs = rng.normal(100,10,n)\n",
    "    means3[i] = obs.mean()\n",
    "    # print (\"Sample \" + str(i) + \", Mean: \", '%.2f' % means[i])\n",
    "print(\"Mean of Sample Means: %.3f\" % np.mean(means3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c573701",
   "metadata": {
    "id": "iqU60dhp479b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(means1, color='green', bins=20,\n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xlim(70,130)\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Dist. of means of 10K samples of size 2')\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(means2,  color='green', bins=20,\n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xlim(70,130)\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Dist. of means of 10K samples of size 3');\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(means3,  color='green', bins=20,\n",
    "         align='left', edgecolor='black', density=True)\n",
    "plt.xlim(70,130)\n",
    "plt.xlabel('Sample Mean')\n",
    "plt.ylabel('Probability');\n",
    "plt.title('Dist. of means of 10K samples of size 6');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a8180",
   "metadata": {
    "id": "fjNyB3CH9YNW"
   },
   "source": [
    "This distribution of the sample means \"tightens up\" as the\n",
    "sample size gets bigger. We've seen this before.\n",
    "\n",
    "We can characterize this meta-distribution in terms of its\n",
    "own mean and standard deviation, although of course we\n",
    "need to estimate those from our sample in real situations.\n",
    "\n",
    "Calculating the mean of a sample equates to drawing one\n",
    "variate from the meta-distribution.\n",
    "\n",
    "Therefore we can ask how often we're likely to see extreme\n",
    "values of the sample mean, i.e., values that lie in the tails of the meta-distribution.\n",
    "\n",
    "**How to calculate the probability of extreme sample means?**\n",
    "\n",
    "If our sample size was big enough, we could use the normal\n",
    "distribution to make this calculation.\n",
    "\n",
    "We would ask how many standard deviations away from the\n",
    "overall mean our particular sample mean was: this is called\n",
    "a Z-score if the distribution is normal.\n",
    "\n",
    "In our case we're going to calculate basically the same thing and call it a t-score because we can't assume normality.\n",
    "\n",
    "**Calculating a t-score** \n",
    "\n",
    "We want to know how many standard deviations away from\n",
    "the overall mean of the sampling distribution our one\n",
    "particular sample is.\n",
    "\n",
    "Let's flesh out the example: we take our measurement scheme\n",
    "to a new country, and we want to know whether the people\n",
    "here are any different than they were at home.\n",
    "\n",
    "This gives us our null and alternative hypotheses:\n",
    "\n",
    "* The null hypothesis is that there's no difference in the measures between the two countries: the mean is 100 in both\n",
    "cases. \n",
    "* Our alternative hypothesis is simply the converse, that there is some difference in measures between the countries.\n",
    "\n",
    "Note, that we usually don't have a commitment about\n",
    "whether the difference, if there is one, will be positive or\n",
    "negative.\n",
    "\n",
    "$H_0$: $\\mu = 100$ \n",
    "\n",
    "\n",
    "We collect a sample of 6 people, and collect measures.\n",
    "\n",
    "They score: 101, 112, 100, 107, 94, 104.\n",
    "\n",
    "We can look at the mean and standard deviation. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9089b",
   "metadata": {
    "id": "_9WBrgrY_pMg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "meas = [101, 112, 100, 107, 94, 104]\n",
    "print(\"Means of samples:   %.3f\" % np.mean(meas))\n",
    "print(\"Std dev of samples: %.3f\" % np.std(meas)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f751447d",
   "metadata": {
    "id": "-UZHhvKMAgf-"
   },
   "source": [
    "But we don't want the plain SD, we want the sample\n",
    "standard deviation (division by N-1) because we're trying to\n",
    "estimate the population standard deviation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749efb3",
   "metadata": {
    "id": "-aaVFuUDAtcR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Sample std dev of samples: %.3f\" % np.std(meas, ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7665ca",
   "metadata": {
    "id": "hPYzoLGcBalw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "se = np.std(meas, ddof=1)/np.sqrt(n)\n",
    "print(\"Standard error :  %.3f\" % se)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b808421",
   "metadata": {
    "id": "GpDCIgf0CP6Q"
   },
   "source": [
    "We now have our best guess at the meta-distribution of the\n",
    "sample-of-size-six means: in the absence of any other\n",
    "information, we'd say that its mean is 103 and its standard\n",
    "deviation is 2.53.\n",
    "\n",
    "However, our null hypothesis is that our six numbers come\n",
    "from a distribution with a mean of 100, i.e., the same as\n",
    "back home.\n",
    "\n",
    "--- \n",
    "\n",
    "We might have helped ourselves to the assumption that the\n",
    "standard deviation of the measures in this new country is 10, the same as at home. But we're not going to do that: who is to\n",
    "say that measures doesn't have a different spread here?\n",
    "\n",
    "So our null hypothesis says: let's imagine that our sample\n",
    "mean comes from a distribution of sample means with mean\n",
    "of 100 and standard deviation of 2.53\n",
    "\n",
    "This gives us our t-statistic: $ t = \\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}} $\n",
    "\n",
    "So a t-score is a lot like a z-score: it's essentially measuring the number of standard deviations from an expected mean that some measurement is.\n",
    "\n",
    "In our case, the t-score is (103 - 100) / 2.53 = 1.18.\n",
    "\n",
    "That's not a great distance from the mean: recall the 1.96\n",
    "threshold for z-scores that equates to the most extreme 5%\n",
    "of the distribution.\n",
    "\n",
    "Similarly, it turns out that a t-score of ±1.18, or a more\n",
    "extreme value, happens 28.9% of the time (this is our pvalue). So we're not motivated to reject $H_0$.\n",
    "\n",
    "#### p-values \n",
    "\n",
    "In the old days you would look up a table of critical p-values for the t-distribution with an appropriate number of degrees of freedom.\n",
    "\n",
    "Degrees of freedom come up a lot in statistics. It's just a\n",
    "measure of how many free parameters something has. For\n",
    "one-sample t-tests, the degrees of freedom are N-1, where\n",
    "N is the sample size. This is because to get a particular\n",
    "value of t, the last score in the sample is not free to vary.\n",
    "\n",
    "We need to specify whether we're interested in a one-tailed\n",
    "or a two-tailed test. \n",
    "\n",
    "A two-tailed test is the default option. This means that we\n",
    "have no strong commitment on whether the sample mean is\n",
    "likely to be higher or lower than the mean specified in the\n",
    "null hypothesis. Thus we include both extreme tails of the\n",
    "distribution when figuring out our p-value.\n",
    "\n",
    "If for some reason we only cared about evidence for an\n",
    "alternative hypothesis that the mean score was (e.g.) higher, we could use a one-tailed test and look at only one side of the t-distribution in figuring out p.\n",
    "\n",
    "Here we can make use of the `scipy` module, in particular the [`ttest_1samp`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html) function. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f80efa",
   "metadata": {
    "id": "MJ6XN9lpEohK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.stats.ttest_1samp(meas, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3396bf3",
   "metadata": {
    "id": "ibesGs5sETtB"
   },
   "source": [
    "### Exercise: Red Wine \n",
    "\n",
    "Let's go back to the pH or our red wines.  First, recompute the population mean and standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd6de3",
   "metadata": {
    "id": "8YVqnueUETtC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu = wine[\"pH\"].mean()\n",
    "sigma = wine[\"pH\"].std()\n",
    "print (\"mean = %0.3f, sigma = %0.4f\" % (mu, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4b637",
   "metadata": {
    "id": "1vse4bj6ETtE"
   },
   "source": [
    "Suppose now, we want to take a sample of 30 wines. Suppose that our hypothesis is that the mean is 3.1. The alternative hypothesis, is that the mean is different from 3.1 (i.e., a two-tail test). Lets sample the population and compute the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0389cb",
   "metadata": {
    "id": "CS5Prr7iETtE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 30\n",
    "obs = np.random.choice(wine.index.values,n)\n",
    "sampled_wines = wine.loc[obs]\n",
    "xbar = sampled_wines[\"pH\"].mean()\n",
    "print (\"Sample mean = %.3f\" % xbar)\n",
    "tval = (xbar - 3.1) / (sigma/np.sqrt(n))\n",
    "print(\"Tstat = %.3f\" % tval)\n",
    "\n",
    "alpha = 0.05\n",
    "pvalue = 2*(1-t.cdf(np.abs(tval), n-1)) \n",
    "# note: the factor of 2 is here because of the two-tailed test\n",
    "\n",
    "print(\"The p-value is %0.8f\"%(pvalue))\n",
    "\n",
    "\n",
    "if (pvalue < alpha):\n",
    "    print (\"We reject the null hypothesis\")\n",
    "else:\n",
    "    print (\"We have no evidence to reject the null hypothesis\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c5fad",
   "metadata": {
    "id": "wSFNbQKUETtH"
   },
   "source": [
    "There are convenient relationships between confidence intervals and hypothesis testing. \n",
    "* Sometimes, we compute a CI for $\\mu$ after rejecting $H_0$ to report the location of plausible values for $\\mu$.\n",
    "* A two-sided hypothesis test with a significance level of $\\alpha$ is\n",
    "equivalent to constructing a $(1- \\alpha)100\\%$ confidence interval for $\\mu$.\n",
    "* We can check whether the CI contains $\\mu_0$:\n",
    "    * If the interval does contain $\\mu_0$, then we fail to reject $H_0$.\n",
    "    * If the interval does not contain $\\mu_0$, then we reject $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae29bc4",
   "metadata": {
    "id": "_VXEsjZpKUnR"
   },
   "source": [
    "### Other tests \n",
    "\n",
    "The one-sample t-test is what we've covered so far.\n",
    "\n",
    "The one-sample test can be used to deal with simple\n",
    "experimental designs in which we measure something\n",
    "before and after an intervention. For example, does drug X\n",
    "lower blood pressure, or does diet Y lead to weight loss?\n",
    "\n",
    "* For each case in the study, we subtract the \"before\" score\n",
    "from the \"after\" score to get a difference\n",
    "* We can then examine the null hypothesis that the mean of\n",
    "the differences is zero, i.e., that the intervention makes no difference. \n",
    "\n",
    "#### Two-sample t-test \n",
    "\n",
    "\n",
    "The two-sample t-test is an extension of the same idea.\n",
    "\n",
    "It is used to test the null hypothesis that two different\n",
    "samples in an experiment are drawn from the same\n",
    "population, i.e., that they have the same mean.\n",
    "\n",
    "For example, does drug A work any better or worse than\n",
    "drug B in reducing blood pressure? Do men and women\n",
    "systematically differ on their measurements?\n",
    "\n",
    "There are some mathematical complications based on\n",
    "whether or not the sample sizes are the same and whether\n",
    "or not we can assume equal variances across the two\n",
    "samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8009529",
   "metadata": {
    "id": "2BvGi8oLKT9G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8d99b",
   "metadata": {
    "id": "vIyP_EsiETtJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f579305",
   "metadata": {
    "id": "Z56LfI5gETtM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "un5550fa24",
   "language": "python",
   "name": "un5550fa24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
