{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "_xo8l1GAQuaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1: Scrape Title\n",
        "\n",
        "**Objective:**\n",
        "Scrape all the titles of the books listed on the Books to Scrape website (http://books.toscrape.com/).\n",
        "\n",
        "**Instructions:**\n",
        "- Use requests to send a GET request to the Books to Scrape homepage.\n",
        "- Parse the HTML content using BeautifulSoup.\n",
        "- Extract the book titles, which are inside \"h3\" tags with a nested \"a\" tag, and print them."
      ],
      "metadata": {
        "id": "p9e1KJo7GGeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL to scrape\n",
        "url = 'https://www.example.com'\n",
        "\n",
        "# Send GET request to the website\n",
        "response = ...\n",
        "\n",
        "# Parse the page content using BeautifulSoup\n",
        "soup = BeautifulSoup(...)\n",
        "\n",
        "# Find all the title tags and print them\n",
        "titles = ...\n",
        "for title in titles:\n",
        "    print(title.text)\n"
      ],
      "metadata": {
        "id": "2ybn-trsG2FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2: Web Scraping (headlines)\n",
        "**Objective:**\n",
        "Scrape headlines from the website https://news.ycombinator.com/.\n",
        "\n",
        "**Task:**\n",
        "- Send a GET request to the URL using requests.\n",
        "- Parse the HTML content with BeautifulSoup.\n",
        "- Extract and print the headlines inside <a> tags with class storylink.\n",
        "Handle cases where no headlines are found."
      ],
      "metadata": {
        "id": "XCKyJMSCFqoD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsepNfqoFep2"
      },
      "outputs": [],
      "source": [
        "# URL of the news site to scrape\n",
        "url = 'https://news.ycombinator.com/'\n",
        "\n",
        "# Send GET request to the website\n",
        "response = ...\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    print(\"Request successful!\")\n",
        "else:\n",
        "    print(f\"Request failed with status code: {response.status_code}\")\n",
        "\n",
        "# Parse the page content using BeautifulSoup\n",
        "soup = BeautifulSoup(...)\n",
        "\n",
        "# Debugging: Print the raw HTML content to see what we are working with\n",
        "print(soup.prettify()[:1000])  # Print first 1000 characters to inspect the HTML\n",
        "\n",
        "# Find all the headline links (they are in <a> tags with class 'storylink')\n",
        "headlines = ...\n",
        "\n",
        "# Print each headline\n",
        "if headlines:\n",
        "    for headline in headlines:\n",
        "        print(headline.text)\n",
        "else:\n",
        "    print(\"No headlines found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. Web Scraping (links)\n",
        "**Objective:**\n",
        "Scrape all the links (from <a> tags) from the Wikipedia homepage (https://www.wikipedia.org/).\n",
        "\n",
        "**Instructions:**\n",
        "- Use requests to send a GET request to the Wikipedia homepage.\n",
        "- Parse the HTML content using BeautifulSoup.\n",
        "- Extract all the links (href attributes) from <a> tags and print them."
      ],
      "metadata": {
        "id": "JAb1un5jG_Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL to scrape\n",
        "url = 'https://www.wikipedia.org/'\n",
        "\n",
        "# Send GET request to the website\n",
        "response = ...\n",
        "\n",
        "# Parse the page content using BeautifulSoup\n",
        "soup = BeautifulSoup(...)\n",
        "\n",
        "# Find all the anchor tags (<a>) which contain links\n",
        "links = ...\n",
        "\n",
        "# Print all the links\n",
        "for link in links:\n",
        "    href = ...\n",
        "    if href:\n",
        "        print(href)\n"
      ],
      "metadata": {
        "id": "ydQcB1rhJJGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. Web Scraping (quote texts)\n",
        "**Objective:**\n",
        "Scrape all the quote texts from the Quotes to Scrape website (https://quotes.toscrape.com/).\n",
        "\n",
        "**Instructions:**\n",
        "- Use requests to send a GET request to the Quotes to Scrape homepage.\n",
        "- Parse the HTML content using BeautifulSoup.\n",
        "- Extract all the quote texts, which are inside <span> tags with the class 'text', and print them."
      ],
      "metadata": {
        "id": "skGaf1FyJL5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the blog to scrape\n",
        "url = 'https://quotes.toscrape.com/'\n",
        "\n",
        "# Send GET request to the website\n",
        "response = ...\n",
        "\n",
        "# Parse the page content using BeautifulSoup\n",
        "soup = BeautifulSoup(...)\n",
        "\n",
        "# Find all the article titles (here, they are in <span> tags with class 'text')\n",
        "titles = soup.find_all(...)\n",
        "\n",
        "# Print each article title\n",
        "for title in titles:\n",
        "    ...\n"
      ],
      "metadata": {
        "id": "6TVRTo1MJN5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Scraping a Simple Website for Titles and Links\n",
        "- We'll scrape a simpler, publicly accessible website like Quotes to Scrape, which is designed for learning purposes.\n",
        "- URL: http://quotes.toscrape.com/"
      ],
      "metadata": {
        "id": "WqKP3IyHKfmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the quotes website\n",
        "url = 'http://quotes.toscrape.com/'\n",
        "\n",
        "# Send GET request to the website\n",
        "response = ...\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == ...:\n",
        "    print(...)\n",
        "else:\n",
        "    print(f\"Request failed with status code: {response.status_code}\")\n",
        "\n",
        "# Parse the page content using BeautifulSoup\n",
        "soup = BeautifulSoup(...)\n",
        "\n",
        "# Find all quote containers (they are in <div> tags with class 'quote')\n",
        "quotes = soup.find_all(...)\n",
        "\n",
        "# Loop through each quote and extract the text and author\n",
        "for quote in quotes:\n",
        "    text = ...\n",
        "    author = ...\n",
        "\n",
        "    print(f\"Quote: {text}\")\n",
        "    print(f\"Author: {author}\")\n",
        "    print('-' * 40)\n"
      ],
      "metadata": {
        "id": "YPRc8mY7Kpar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6: Web Scraping eBay for Product Listings\n",
        "\n",
        "**Objective:** Write a Python script to scrape eBay search results for a specific product (e.g., laptops), extract product details, and display them.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- Use the requests library to send an HTTP GET request to eBayâ€™s search results page for a given product (e.g., \"laptop\").\n",
        "- Use the BeautifulSoup library to parse the HTML content of the response.\n",
        "- Extract the following details for each product listing:\n",
        "  - Title: The name or description of the product.\n",
        "  - Price: The price of the product.\n",
        "  - Hit Count: The number of views or watchers for the product.\n",
        "  - Display the extracted data in a readable format with product index, title, price, and hit count.\n",
        "  - Handle cases where some data may be missing by providing default values."
      ],
      "metadata": {
        "id": "C0Rd5cChStdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URL to scrape (Example: eBay search results for \"laptop\")\n",
        "url = \"https://www.ebay.com/sch/i.html?_nkw=laptop\"\n",
        "\n",
        "# Set headers to mimic a real browser request\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "# Send an HTTP GET request to the website\n",
        "response = requests.get(...)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == ...:\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(...)\n",
        "\n",
        "    # Extract product listings\n",
        "    products = soup.find_all('li', class_='s-item')\n",
        "\n",
        "    extracted_data = []\n",
        "    for idx, product in enumerate(products, start=1):\n",
        "        title_tag = product.find(...)\n",
        "        price_tag = product.find(...)\n",
        "        hit_count_tag = product.find(...)  # eBay shows watchers/views here\n",
        "\n",
        "        title = title_tag.text.strip() if title_tag else 'No Title'\n",
        "        price = price_tag.text.strip() if price_tag else 'No Price'\n",
        "        hits = hit_count_tag.text.strip() if hit_count_tag else 'No Hit Count'\n",
        "\n",
        "        extracted_data.append({\"index\": idx, \"title\": title, \"price\": price, \"hits\": hits})\n",
        "\n",
        "    # Print extracted data\n",
        "    for data in extracted_data:\n",
        "        print(f\"...\")\n",
        "else:\n",
        "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "B9NhEl_2SuTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}